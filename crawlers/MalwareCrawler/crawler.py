import Queue
import logging
import socket
import threading
import urllib2

import os
import re
import requests
from BeautifulSoup import BeautifulSoup

from config import *

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
download_queue = Queue.Queue()


class DownloaderThread(threading.Thread):

    def __init__(self):
        threading.Thread.__init__(self)

    def run(self):
        download_apps()


def download_apps():
    while not download_queue.empty():
        link = download_queue.get()
        try:
            logger.info("Downloading from " + link)
            downloaded_file = urllib2.urlopen(link)
            name = downloaded_file.info()['Content-Disposition'].split(
                "filename=")
            name = name[1].split(";")[0].replace('\"', '').split(".zip")[0]
            file_name = DOWNLOAD_PATH + name + ".zip"
            data = downloaded_file.read()
            password = BASE_PASSWORD + name[len(name) - 1]
            if not os.path.isfile(file_name):
                open(file_name, "wb").write(data)
                os.system(
                    "unzip -P " + '\"' + password + "\" " + file_name + " "
                    + "-d " + SAVE_PATH)
                logger.info("Extracting  " + file_name)
                if os.path.isdir(SAVE_PATH + name):
                    os.system("mv " + SAVE_PATH + name + "/* " + SAVE_PATH)
        except urllib2.HTTPError, e:
            logger.exception(e.code)
        except socket.timeout:
            logger.exception("Timed Out")


def get_malware_links():
    url = BASE_URL
    while download_queue.qsize() <= NUMBER:
        logger.info("Getting " + url)
        page = requests.get(url)
        page_html = page.text
        soup = BeautifulSoup(page_html)
        anchor = soup.findAll(name='a', href=re.compile("dl=0$"))
        logger.info("Parsing " + url)
        for a in anchor:
            link = str(a['href'])
            link = link.replace("dl=0", "dl=1")
            download_queue.put(link)
        url = soup.find(name='a', attrs={'class': 'blog-pager-older-link'})
        if url:
            url = str(url['href'])
        else:
            return


if __name__ == '__main__':
    get_malware_links()
    thread1 = DownloaderThread()
    thread2 = DownloaderThread()
    thread3 = DownloaderThread()
    thread4 = DownloaderThread()
    thread5 = DownloaderThread()

    thread1.start()
    thread2.start()
    thread3.start()
    thread4.start()
    thread5.start()
